{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARKIFY PROJECT - MODELING\n",
    "**This notebook explores a tiny subset (128MB)** of the full dataset available (12GB).  \n",
    "Both can be retrieved here:\n",
    "* 128MB subset: [s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json](s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json)\n",
    "* full 12GB dataset: [s3n://udacity-dsnd/sparkify/sparkify_event_data.json](s3n://udacity-dsnd/sparkify/sparkify_event_data.json)\n",
    "\n",
    "After the ***Data Understanding*** phase done in this [notebook](1_Sparkify_Data_Understanding.ipynb) and the ***Data Exploration*** phase done in this [notebook](2_Sparkify_Data_Exploration.ipynb) and the , we are now entering the ***Modeling phase***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, init Spark and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "\n",
    "from pyspark.sql.functions import udf, desc, isnan, when, count, col, lit\n",
    "from pyspark.sql.functions import max as Fmax\n",
    "from pyspark.sql.functions import mean as Fmean\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import re\n",
    "\n",
    "# MODELING imports\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is useful to know the version we are using when reading the pyspark documentations\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or retrieve a Spark session\n",
    "spark = SparkSession.builder.appName(\"dsnd-p7-sparkify\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+-----------------+------+-------------+--------------------+------+\n",
      "|          artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|             song|status|           ts|           userAgent|userId|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+-----------------+------+-------------+--------------------+------+\n",
      "|  Martha Tilston|Logged In|    Colin|     M|           50| Freeman|277.89016| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|        Rockpools|   200|1538352117000|Mozilla/5.0 (Wind...|    30|\n",
      "|Five Iron Frenzy|Logged In|    Micah|     M|           79|    Long|236.09424| free|Boston-Cambridge-...|   PUT|NextSong|1538331630000|        8|           Canada|   200|1538352180000|\"Mozilla/5.0 (Win...|     9|\n",
      "|    Adam Lambert|Logged In|    Colin|     M|           51| Freeman| 282.8273| paid|     Bakersfield, CA|   PUT|NextSong|1538173362000|       29|Time For Miracles|   200|1538352394000|Mozilla/5.0 (Wind...|    30|\n",
      "+----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+-----------------+------+-------------+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"mini_sparkify_event_data.json\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pyspark dataframe has shape (286500, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded pyspark dataframe has shape ({}, {})\".format(df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN: remove empty users and build the `churn` target feature\n",
    "We copy the code from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned pyspark dataframe has now shape (278154, 18)\n",
      "Pyspark dataframe has now shape (278154, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_users = df.filter((~(isnan(df['userId']))) & (df['userId'].isNotNull()) & (df['userId'] != \"\"))\n",
    "print(\"Cleaned pyspark dataframe has now shape ({}, {})\".format(df_clean_users.count(), len(df_clean_users.columns)))\n",
    "\n",
    "# Define the UDF (do not forget to precise type of column otherwise String is taken by default)\n",
    "user_has_churned = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "\n",
    "# Apply this function on a specific column of the whole dataset\n",
    "# (made with the help of: https://gist.github.com/zoltanctoth/2deccd69e3d1cde1dd78\n",
    "# and https://docs.databricks.com/spark/latest/spark-sql/udf-python.html)\n",
    "df_users_with_churn = df_clean_users.withColumn(\"churn\", user_has_churned(\"page\"))\n",
    "\n",
    "print(\"Pyspark dataframe has now shape ({}, {})\".format(df_users_with_churn.count(), len(df_users_with_churn.columns)))\n",
    "\n",
    "df_users_with_churn_full = df_users_with_churn.withColumn(\"churn\", Fmax('churn').over(Window.partitionBy(\"userId\")))\n",
    "\n",
    "# Check that we are still talking about our 52 users\n",
    "df_users_with_churn_full.filter(df_users_with_churn_full['churn'] == 1).select('userId').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. FEATURE ENGINEERING\n",
    "In the previous notebook, the conclusion after all observations and plots was to build some new features based on those observations as they could help the model to detect churn. As a reminder it was:\n",
    "* Transform as binary 0/1 the `level` of subscription (paid or not)\n",
    "* we can dummy the `gender` (binary 0/1 as well)\n",
    "* `registration` time for the user\n",
    "* `engagement` of the user with the number of artists, songs or even the total length of music listened, add to playlist number\n",
    "* `social interactions` with likes/dislikes, friends, etc\n",
    "* `upgrade/downgrade` the subscription level\n",
    "* `user operating system` which could help us to identify users of a version that does not give entire satisfaction\n",
    "* `errors_encountered` which could help us to identify users who had several issues and then maybe quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Keep only useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataframe has shape (278154, 13)\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df_users_with_churn_full.select(['artist', 'gender', 'length', 'level', 'page', 'registration', 'sessionId', 'song', 'status', 'ts', 'userAgent', 'userId', 'churn'])\n",
    "print(\"Filtered dataframe has shape ({}, {})\".format(df_filtered.count(), len(df_filtered.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Transform `level` and `gender` into binary 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---------+-----+---------+-------------+---------+--------------------+------+-------------+--------------------+------+-----+\n",
      "|              artist|gender|   length|level|     page| registration|sessionId|                song|status|           ts|           userAgent|userId|churn|\n",
      "+--------------------+------+---------+-----+---------+-------------+---------+--------------------+------+-------------+--------------------+------+-----+\n",
      "|Sleeping With Sirens|     0|202.97098|    0| NextSong|1538016340000|       31|Captain Tyin Knot...|   200|1539003534000|\"Mozilla/5.0 (iPh...|100010|    0|\n",
      "|Francesca Battist...|     0|196.54485|    0| NextSong|1538016340000|       31|Beautiful_ Beauti...|   200|1539003736000|\"Mozilla/5.0 (iPh...|100010|    0|\n",
      "|              Brutha|     0|263.13098|    0| NextSong|1538016340000|       31|          She's Gone|   200|1539003932000|\"Mozilla/5.0 (iPh...|100010|    0|\n",
      "|                null|     0|     null|    0|Thumbs Up|1538016340000|       31|                null|   307|1539003933000|\"Mozilla/5.0 (iPh...|100010|    0|\n",
      "|         Josh Ritter|     0|316.23791|    0| NextSong|1538016340000|       31|      Folk Bloodbath|   200|1539004195000|\"Mozilla/5.0 (iPh...|100010|    0|\n",
      "+--------------------+------+---------+-----+---------+-------------+---------+--------------------+------+-------------+--------------------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the UDF (do not forget to precise type of column otherwise String is taken by default)\n",
    "to_dummy_level = udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
    "to_dummy_gender = udf(lambda x: 1 if x == \"M\" else 0, IntegerType())\n",
    "\n",
    "df_filtered = df_filtered.withColumn(\"level\", to_dummy_level(\"level\"))\n",
    "df_filtered = df_filtered.withColumn(\"gender\", to_dummy_level(\"gender\"))\n",
    "df_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lvl = df_filtered.select('userId', 'level').groupby('userId').agg({'level': 'max'}).withColumnRenamed('max(level)', 'level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Count number of days the service has been used by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|userId|timedelta|\n",
      "+------+---------+\n",
      "|100010|       55|\n",
      "|200002|       70|\n",
      "|   125|       71|\n",
      "|    51|       19|\n",
      "|   124|      131|\n",
      "+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the UDF (do not forget to precise type of column otherwise String is taken by default)\n",
    "to_delta_in_days = udf(lambda x: timedelta(seconds=x).days, IntegerType())\n",
    "\n",
    "# Build a time diff column\n",
    "df_time_delta = df_filtered.select('userId', 'registration', 'ts').withColumn('timedelta', (df_filtered.ts - df_filtered.registration)/1000)\n",
    "df_time_delta = df_time_delta.withColumn(\"timedelta\", to_delta_in_days(\"timedelta\"))\n",
    "\n",
    "# Keep only the max per user\n",
    "df_time_delta = df_time_delta.select('userId', 'timedelta').groupBy('userId').agg({'timedelta': 'max'}).withColumnRenamed('max(timedelta)', 'timedelta')\n",
    "df_time_delta.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Measure engagement of the user with the number of artists/songs, total length, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nb_artist_songs(df, col_name, count_unique):\n",
    "    \"\"\"\n",
    "    Count the number of artists or songs listened per user (the count is distinct if count_unique is True)\n",
    "    \"\"\"\n",
    "    innerdf = df.filter(df_filtered[col_name] != 'null').select('userId', col_name)\n",
    "    if count_unique:\n",
    "        innerdf = innerdf.dropDuplicates()\n",
    "    return innerdf.groupBy('userId').count().withColumnRenamed('count', 'nb_{}_{}s'.format('unique' if count_unique else 'total', col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nb_page(df, page_value, total_col_name):\n",
    "    \"\"\"\n",
    "    Build a new dataframe filtered on a certain page value and count the number of times each user has seen this page\n",
    "    \"\"\"\n",
    "    return df.filter(df['page'] == page_value).select(['page', 'userId']).groupBy('userId').agg({'page': 'count'}).withColumnRenamed('count(page)', total_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_unique_songs = count_nb_artist_songs(df_filtered, 'song', True)\n",
    "df_users_total_songs = count_nb_artist_songs(df_filtered, 'song', False)\n",
    "df_users_unique_artists = count_nb_artist_songs(df_filtered, 'artist', True)\n",
    "df_users_total_artists = count_nb_artist_songs(df_filtered, 'artist', False)\n",
    "\n",
    "df_total_length = df_filtered.select('userId', 'length').groupBy('userId').agg({'length': 'sum'}).withColumnRenamed('sum(length)', 'total_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_playlist = count_nb_page(df_filtered, 'Add to Playlist', 'total_add_playlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Measure social interactions with likes/dislikes, add friend, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_friend = count_nb_page(df_filtered, 'Add Friend', 'total_add_friend')\n",
    "df_thumbs_up = count_nb_page(df_filtered, 'Thumbs Up', 'total_thumbs_up')\n",
    "df_thumbs_down = count_nb_page(df_filtered, 'Thumbs Down', 'total_thumbs_down')\n",
    "df_rolling_ads = count_nb_page(df_filtered, 'Roll Advert', 'total_ads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. `Upgrade/Downgrade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_view_upgrade = count_nb_page(df_filtered, 'Upgrade', 'think_upgrade')\n",
    "df_count_upgrade = count_nb_page(df_filtered, 'Submit Upgrade', 'has_upgraded')\n",
    "df_view_downgrade = count_nb_page(df_filtered, 'Downgrade', 'think_downgrade')\n",
    "df_count_downgrade = count_nb_page(df_filtered, 'Submit Downgrade', 'has_downgraded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Extract Operating System information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex (regex101 is your friend to validate it works!)\n",
    "def extract_systeminfo(txt):\n",
    "    matches = re.match(\".*Mozilla/[0-9.]+\\s\\(([a-zA-Z0-9\\s.]+)(;|\\))\", txt)\n",
    "    if matches:\n",
    "        return matches.group(1)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "to_os = udf(lambda x: extract_systeminfo(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------+\n",
      "|userId|           userAgent|            os|\n",
      "+------+--------------------+--------------+\n",
      "|    30|Mozilla/5.0 (Wind...|Windows NT 6.1|\n",
      "|     9|\"Mozilla/5.0 (Win...|Windows NT 6.1|\n",
      "|    30|Mozilla/5.0 (Wind...|Windows NT 6.1|\n",
      "|     9|\"Mozilla/5.0 (Win...|Windows NT 6.1|\n",
      "|    30|Mozilla/5.0 (Wind...|Windows NT 6.1|\n",
      "|     9|\"Mozilla/5.0 (Win...|Windows NT 6.1|\n",
      "|     9|\"Mozilla/5.0 (Win...|Windows NT 6.1|\n",
      "|    30|Mozilla/5.0 (Wind...|Windows NT 6.1|\n",
      "|    30|Mozilla/5.0 (Wind...|Windows NT 6.1|\n",
      "|    30|Mozilla/5.0 (Wind...|Windows NT 6.1|\n",
      "+------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_os = df_filtered.select('userId', 'userAgent').withColumn(\"os\", to_os(\"userAgent\"))\n",
    "df_os.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_os_col(val_to_replace, replaced_value):\n",
    "    \"\"\"\n",
    "    Rename a value within the 'os' column by another one\n",
    "    \"\"\"\n",
    "    return df_os.withColumn(\"os\", when(df_os.os == val_to_replace, lit(replaced_value)).otherwise(df_os.os))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------+\n",
      "|userId|           userAgent|           os|\n",
      "+------+--------------------+-------------+\n",
      "|    30|Mozilla/5.0 (Wind...|Windows Seven|\n",
      "|     9|\"Mozilla/5.0 (Win...|Windows Seven|\n",
      "|    30|Mozilla/5.0 (Wind...|Windows Seven|\n",
      "|     9|\"Mozilla/5.0 (Win...|Windows Seven|\n",
      "|    30|Mozilla/5.0 (Wind...|Windows Seven|\n",
      "|     9|\"Mozilla/5.0 (Win...|Windows Seven|\n",
      "|     9|\"Mozilla/5.0 (Win...|Windows Seven|\n",
      "|    30|Mozilla/5.0 (Wind...|Windows Seven|\n",
      "|    30|Mozilla/5.0 (Wind...|Windows Seven|\n",
      "|    30|Mozilla/5.0 (Wind...|Windows Seven|\n",
      "+------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_os = rename_os_col('compatible', 'Windows NT 6.1')\n",
    "df_os = rename_os_col('X11', 'Linux')\n",
    "df_os = rename_os_col('Windows NT 5.1', 'Windows XP')\n",
    "df_os = rename_os_col('Windows NT 6.0', 'Windows Vista')\n",
    "df_os = rename_os_col('Windows NT 6.1', 'Windows Seven')\n",
    "df_os = rename_os_col('Windows NT 6.2', 'Windows 8')\n",
    "df_os = rename_os_col('Windows NT 6.3', 'Windows 81')\n",
    "df_os.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to build dummies? Found something on SO: https://stackoverflow.com/questions/46528207/dummy-encoding-using-pyspark\n",
    "df_os_tmp = df_os.groupby('userId').agg({'os': 'max'}).withColumnRenamed('max(os)', 'os')\n",
    "os_list = df_os_tmp.select('os').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "exprs = [when(col('os') == os, 1).otherwise(0).alias(str(os)) for os in os_list]\n",
    "df_os_tmp = df_os_tmp.select(exprs + df_os_tmp.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Count number of errors per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors = count_nb_page(df_filtered, 'Error', 'nb_404')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9. Merge everything into a single final dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a series of different issues I have decided to transform each subset into pandas before doing the join that I hope will be faster.  \n",
    "The main issue was about some timeouts due to computation time. I have read few things on Stackoverflow and changed some configuration parameters (for example [this](https://stackoverflow.com/questions/43984068/does-spark-sql-autobroadcastjointhreshold-work-for-joins-using-datasets-join-op)) and set:  \n",
    "```spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)```\n",
    "\n",
    "But anyway it still did not work, maybe due to the power of the machine it runs on or because I do something wrong as I am new with Spark. Maybe the too many select/filters and columns manipulations ends with something too big to be played on this kind of machine (the explain plan was very impressive so I guess this is the root cause).\n",
    "\n",
    "Anyway, as we know that we have only few users, it will fit within classic pandas DataFrame. So let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 286 ms, sys: 61.1 ms, total: 347 ms\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save as pandas for the join later (will be much much mush faster...)\n",
    "df_pd_users = df_filtered.select('userId', 'churn', 'gender').dropDuplicates().toPandas()\n",
    "df_pd_lvl = df_lvl.toPandas()\n",
    "df_pd_time_delta = df_time_delta.toPandas()\n",
    "df_pd_users_unique_songs = df_users_unique_songs.toPandas()\n",
    "df_pd_users_total_songs = df_users_total_songs.toPandas()\n",
    "df_pd_users_unique_artists = df_users_unique_artists.toPandas()\n",
    "df_pd_total_length = df_total_length.toPandas()\n",
    "df_pd_add_playlist = df_add_playlist.toPandas()\n",
    "df_pd_add_friend = df_add_friend.toPandas()\n",
    "df_pd_thumbs_up = df_thumbs_up.toPandas()\n",
    "df_pd_thumbs_down = df_thumbs_down.toPandas()\n",
    "df_pd_rolling_ads = df_rolling_ads.toPandas()\n",
    "df_pd_view_upgrade = df_view_upgrade.toPandas()\n",
    "df_pd_count_upgrade = df_count_upgrade.toPandas()\n",
    "df_pd_view_downgrade = df_view_downgrade.toPandas()\n",
    "df_pd_count_downgrade = df_count_downgrade.toPandas()\n",
    "df_pd_errors = df_errors.toPandas()\n",
    "df_pd_os = df_os_tmp.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 3)\n",
      "(225, 2)\n",
      "(225, 2)\n",
      "(225, 2)\n",
      "(225, 2)\n",
      "(225, 2)\n",
      "(225, 2)\n",
      "(215, 2)\n",
      "(206, 2)\n",
      "(220, 2)\n",
      "(203, 2)\n",
      "(207, 2)\n",
      "(168, 2)\n",
      "(131, 2)\n",
      "(154, 2)\n",
      "(49, 2)\n",
      "(117, 2)\n",
      "(225, 11)\n"
     ]
    }
   ],
   "source": [
    "df_list = [df_pd_users, df_pd_lvl, df_pd_time_delta, df_pd_users_unique_songs, df_pd_users_total_songs, \n",
    "           df_pd_users_unique_artists, df_pd_total_length, df_pd_add_playlist, \n",
    "           df_pd_add_friend, df_pd_thumbs_up, df_pd_thumbs_down, df_pd_rolling_ads, df_pd_view_upgrade, \n",
    "           df_pd_count_upgrade, df_pd_view_downgrade, df_pd_count_downgrade, df_pd_errors, df_pd_os]\n",
    "for a_df in df_list:\n",
    "    print(a_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_final = df_pd_users\n",
    "for a_df in df_list[1:]:\n",
    "    df_pd_final = df_pd_final.merge(a_df, on='userId', how='left')\n",
    "# In the end remove the userId that is now useless and fill potential NaN with 0's\n",
    "df_pd_final = df_pd_final.drop(['userId', 'os'], axis=1)\n",
    "df_pd_final = df_pd_final.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 27)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn</th>\n",
       "      <th>gender</th>\n",
       "      <th>level</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>nb_unique_songs</th>\n",
       "      <th>nb_total_songs</th>\n",
       "      <th>nb_unique_artists</th>\n",
       "      <th>total_length</th>\n",
       "      <th>total_add_playlist</th>\n",
       "      <th>total_add_friend</th>\n",
       "      <th>...</th>\n",
       "      <th>nb_404</th>\n",
       "      <th>Windows 8</th>\n",
       "      <th>iPad</th>\n",
       "      <th>iPhone</th>\n",
       "      <th>Macintosh</th>\n",
       "      <th>Linux</th>\n",
       "      <th>Windows Vista</th>\n",
       "      <th>Windows 81</th>\n",
       "      <th>Windows XP</th>\n",
       "      <th>Windows Seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>626</td>\n",
       "      <td>667</td>\n",
       "      <td>545</td>\n",
       "      <td>165790.33923</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>542</td>\n",
       "      <td>583</td>\n",
       "      <td>461</td>\n",
       "      <td>144836.59439</td>\n",
       "      <td>26.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>7079.69297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>1539</td>\n",
       "      <td>1728</td>\n",
       "      <td>1163</td>\n",
       "      <td>429447.28834</td>\n",
       "      <td>53.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>349</td>\n",
       "      <td>367</td>\n",
       "      <td>317</td>\n",
       "      <td>93505.00914</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>1012</td>\n",
       "      <td>1098</td>\n",
       "      <td>824</td>\n",
       "      <td>273173.73246</td>\n",
       "      <td>37.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>2562</td>\n",
       "      <td>3028</td>\n",
       "      <td>1804</td>\n",
       "      <td>754517.56257</td>\n",
       "      <td>89.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>980</td>\n",
       "      <td>1045</td>\n",
       "      <td>806</td>\n",
       "      <td>260526.52036</td>\n",
       "      <td>38.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>1076</td>\n",
       "      <td>1169</td>\n",
       "      <td>861</td>\n",
       "      <td>289959.35898</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>2300</td>\n",
       "      <td>2676</td>\n",
       "      <td>1672</td>\n",
       "      <td>664572.01781</td>\n",
       "      <td>77.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   churn  gender  level  timedelta  nb_unique_songs  nb_total_songs  \\\n",
       "0      0       0      1         69              626             667   \n",
       "1      0       0      1        100              542             583   \n",
       "2      0       0      0        100               29              29   \n",
       "3      0       0      1         95             1539            1728   \n",
       "4      1       0      0         48              349             367   \n",
       "5      0       0      1        138             1012            1098   \n",
       "6      1       0      1         60             2562            3028   \n",
       "7      0       0      1        125              980            1045   \n",
       "8      1       0      1         75             1076            1169   \n",
       "9      0       0      1         60             2300            2676   \n",
       "\n",
       "   nb_unique_artists  total_length  total_add_playlist  total_add_friend  \\\n",
       "0                545  165790.33923                12.0              14.0   \n",
       "1                461  144836.59439                26.0              11.0   \n",
       "2                 29    7079.69297                 0.0               7.0   \n",
       "3               1163  429447.28834                53.0              28.0   \n",
       "4                317   93505.00914                11.0              15.0   \n",
       "5                824  273173.73246                37.0              14.0   \n",
       "6               1804  754517.56257                89.0              47.0   \n",
       "7                806  260526.52036                38.0              24.0   \n",
       "8                861  289959.35898                32.0              10.0   \n",
       "9               1672  664572.01781                77.0              40.0   \n",
       "\n",
       "       ...        nb_404  Windows 8  iPad  iPhone  Macintosh  Linux  \\\n",
       "0      ...           0.0          0     0       0          0      0   \n",
       "1      ...           1.0          0     0       0          0      0   \n",
       "2      ...           0.0          0     0       0          1      0   \n",
       "3      ...           2.0          0     0       0          1      0   \n",
       "4      ...           0.0          0     0       1          0      0   \n",
       "5      ...           0.0          0     0       0          1      0   \n",
       "6      ...           0.0          0     0       0          1      0   \n",
       "7      ...           0.0          0     0       0          0      0   \n",
       "8      ...           0.0          0     0       0          1      0   \n",
       "9      ...           3.0          0     0       0          0      0   \n",
       "\n",
       "   Windows Vista  Windows 81  Windows XP  Windows Seven  \n",
       "0              0           0           0              1  \n",
       "1              0           0           0              1  \n",
       "2              0           0           0              0  \n",
       "3              0           0           0              0  \n",
       "4              0           0           0              0  \n",
       "5              0           0           0              0  \n",
       "6              0           0           0              0  \n",
       "7              0           0           0              1  \n",
       "8              0           0           0              0  \n",
       "9              0           0           0              1  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally so that it can be reused later\n",
    "df_pd_final.to_csv('df_final_subset128.csv', header=1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn</th>\n",
       "      <th>gender</th>\n",
       "      <th>level</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>nb_unique_songs</th>\n",
       "      <th>nb_total_songs</th>\n",
       "      <th>nb_unique_artists</th>\n",
       "      <th>total_length</th>\n",
       "      <th>total_add_playlist</th>\n",
       "      <th>total_add_friend</th>\n",
       "      <th>...</th>\n",
       "      <th>nb_404</th>\n",
       "      <th>Windows 8</th>\n",
       "      <th>iPad</th>\n",
       "      <th>iPhone</th>\n",
       "      <th>Macintosh</th>\n",
       "      <th>Linux</th>\n",
       "      <th>Windows Vista</th>\n",
       "      <th>Windows 81</th>\n",
       "      <th>Windows XP</th>\n",
       "      <th>Windows Seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>626</td>\n",
       "      <td>667</td>\n",
       "      <td>545</td>\n",
       "      <td>165790.33923</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>542</td>\n",
       "      <td>583</td>\n",
       "      <td>461</td>\n",
       "      <td>144836.59439</td>\n",
       "      <td>26.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>7079.69297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>1539</td>\n",
       "      <td>1728</td>\n",
       "      <td>1163</td>\n",
       "      <td>429447.28834</td>\n",
       "      <td>53.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>349</td>\n",
       "      <td>367</td>\n",
       "      <td>317</td>\n",
       "      <td>93505.00914</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>1012</td>\n",
       "      <td>1098</td>\n",
       "      <td>824</td>\n",
       "      <td>273173.73246</td>\n",
       "      <td>37.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>2562</td>\n",
       "      <td>3028</td>\n",
       "      <td>1804</td>\n",
       "      <td>754517.56257</td>\n",
       "      <td>89.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>980</td>\n",
       "      <td>1045</td>\n",
       "      <td>806</td>\n",
       "      <td>260526.52036</td>\n",
       "      <td>38.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>1076</td>\n",
       "      <td>1169</td>\n",
       "      <td>861</td>\n",
       "      <td>289959.35898</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>2300</td>\n",
       "      <td>2676</td>\n",
       "      <td>1672</td>\n",
       "      <td>664572.01781</td>\n",
       "      <td>77.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   churn  gender  level  timedelta  nb_unique_songs  nb_total_songs  \\\n",
       "0      0       0      1         69              626             667   \n",
       "1      0       0      1        100              542             583   \n",
       "2      0       0      0        100               29              29   \n",
       "3      0       0      1         95             1539            1728   \n",
       "4      1       0      0         48              349             367   \n",
       "5      0       0      1        138             1012            1098   \n",
       "6      1       0      1         60             2562            3028   \n",
       "7      0       0      1        125              980            1045   \n",
       "8      1       0      1         75             1076            1169   \n",
       "9      0       0      1         60             2300            2676   \n",
       "\n",
       "   nb_unique_artists  total_length  total_add_playlist  total_add_friend  \\\n",
       "0                545  165790.33923                12.0              14.0   \n",
       "1                461  144836.59439                26.0              11.0   \n",
       "2                 29    7079.69297                 0.0               7.0   \n",
       "3               1163  429447.28834                53.0              28.0   \n",
       "4                317   93505.00914                11.0              15.0   \n",
       "5                824  273173.73246                37.0              14.0   \n",
       "6               1804  754517.56257                89.0              47.0   \n",
       "7                806  260526.52036                38.0              24.0   \n",
       "8                861  289959.35898                32.0              10.0   \n",
       "9               1672  664572.01781                77.0              40.0   \n",
       "\n",
       "       ...        nb_404  Windows 8  iPad  iPhone  Macintosh  Linux  \\\n",
       "0      ...           0.0          0     0       0          0      0   \n",
       "1      ...           1.0          0     0       0          0      0   \n",
       "2      ...           0.0          0     0       0          1      0   \n",
       "3      ...           2.0          0     0       0          1      0   \n",
       "4      ...           0.0          0     0       1          0      0   \n",
       "5      ...           0.0          0     0       0          1      0   \n",
       "6      ...           0.0          0     0       0          1      0   \n",
       "7      ...           0.0          0     0       0          0      0   \n",
       "8      ...           0.0          0     0       0          1      0   \n",
       "9      ...           3.0          0     0       0          0      0   \n",
       "\n",
       "   Windows Vista  Windows 81  Windows XP  Windows Seven  \n",
       "0              0           0           0              1  \n",
       "1              0           0           0              1  \n",
       "2              0           0           0              0  \n",
       "3              0           0           0              0  \n",
       "4              0           0           0              0  \n",
       "5              0           0           0              0  \n",
       "6              0           0           0              0  \n",
       "7              0           0           0              1  \n",
       "8              0           0           0              0  \n",
       "9              0           0           0              1  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "pd.read_csv('df_final_subset128.csv', header=0).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. MODELING\n",
    "To be able to start from here, the result of the `Feature Engineering` phase has been saved into a CSV file. So let's start by loading it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+---------+---------------+--------------+-----------------+------------------+------------------+----------------+---------------+-----------------+---------+-------------+------------+---------------+--------------+------+---------+----+------+---------+-----+-------------+----------+----------+-------------+\n",
      "|churn|gender|level|timedelta|nb_unique_songs|nb_total_songs|nb_unique_artists|      total_length|total_add_playlist|total_add_friend|total_thumbs_up|total_thumbs_down|total_ads|think_upgrade|has_upgraded|think_downgrade|has_downgraded|nb_404|Windows 8|iPad|iPhone|Macintosh|Linux|Windows Vista|Windows 81|Windows XP|Windows Seven|\n",
      "+-----+------+-----+---------+---------------+--------------+-----------------+------------------+------------------+----------------+---------------+-----------------+---------+-------------+------------+---------------+--------------+------+---------+----+------+---------+-----+-------------+----------+----------+-------------+\n",
      "|    0|     0|    1|       69|            626|           667|              545|165790.33922999998|              12.0|            14.0|           30.0|              6.0|     13.0|          2.0|         1.0|            2.0|           0.0|   0.0|        0|   0|     0|        0|    0|            0|         0|         0|            1|\n",
      "|    0|     0|    1|      100|            542|           583|              461|144836.59438999987|              26.0|            11.0|           56.0|              4.0|      2.0|          1.0|         1.0|            5.0|           0.0|   1.0|        0|   0|     0|        0|    0|            0|         0|         0|            1|\n",
      "|    0|     0|    0|      100|             29|            29|               29| 7079.692970000001|               0.0|             7.0|            2.0|              0.0|      4.0|          1.0|         0.0|            0.0|           0.0|   0.0|        0|   0|     0|        1|    0|            0|         0|         0|            0|\n",
      "+-----+------+-----+---------+---------------+--------------+-----------------+------------------+------------------+----------------+---------------+-----------------+---------+-------------+------------+---------------+--------------+------+---------+----+------+---------+-----+-------------+----------+----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"df_final_subset128.csv\", header=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- churn: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- timedelta: string (nullable = true)\n",
      " |-- nb_unique_songs: string (nullable = true)\n",
      " |-- nb_total_songs: string (nullable = true)\n",
      " |-- nb_unique_artists: string (nullable = true)\n",
      " |-- total_length: string (nullable = true)\n",
      " |-- total_add_playlist: string (nullable = true)\n",
      " |-- total_add_friend: string (nullable = true)\n",
      " |-- total_thumbs_up: string (nullable = true)\n",
      " |-- total_thumbs_down: string (nullable = true)\n",
      " |-- total_ads: string (nullable = true)\n",
      " |-- think_upgrade: string (nullable = true)\n",
      " |-- has_upgraded: string (nullable = true)\n",
      " |-- think_downgrade: string (nullable = true)\n",
      " |-- has_downgraded: string (nullable = true)\n",
      " |-- nb_404: string (nullable = true)\n",
      " |-- Windows 8: string (nullable = true)\n",
      " |-- iPad: string (nullable = true)\n",
      " |-- iPhone: string (nullable = true)\n",
      " |-- Macintosh: string (nullable = true)\n",
      " |-- Linux: string (nullable = true)\n",
      " |-- Windows Vista: string (nullable = true)\n",
      " |-- Windows 81: string (nullable = true)\n",
      " |-- Windows XP: string (nullable = true)\n",
      " |-- Windows Seven: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All columns are read as string, this will be an issue for the modeling part. Columns are casted to their appropriate types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"churn\", df[\"churn\"].cast(\"int\"))\n",
    "df = df.withColumn(\"gender\", df[\"gender\"].cast(\"int\"))\n",
    "df = df.withColumn(\"level\", df[\"level\"].cast(\"int\"))\n",
    "df = df.withColumn(\"timedelta\", df[\"timedelta\"].cast(\"int\"))\n",
    "df = df.withColumn(\"nb_unique_songs\", df[\"nb_unique_songs\"].cast(\"int\"))\n",
    "df = df.withColumn(\"nb_total_songs\", df[\"nb_total_songs\"].cast(\"int\"))\n",
    "df = df.withColumn(\"nb_unique_artists\", df[\"nb_unique_artists\"].cast(\"int\"))\n",
    "df = df.withColumn(\"total_length\", df[\"total_length\"].cast(\"double\"))\n",
    "df = df.withColumn(\"total_add_playlist\", df[\"total_add_playlist\"].cast(\"int\"))\n",
    "df = df.withColumn(\"total_add_friend\", df[\"total_add_friend\"].cast(\"int\"))\n",
    "df = df.withColumn(\"total_thumbs_up\", df[\"total_thumbs_up\"].cast(\"int\"))\n",
    "df = df.withColumn(\"total_thumbs_down\", df[\"total_thumbs_down\"].cast(\"int\"))\n",
    "df = df.withColumn(\"total_ads\", df[\"total_ads\"].cast(\"int\"))\n",
    "df = df.withColumn(\"think_upgrade\", df[\"think_upgrade\"].cast(\"int\"))\n",
    "df = df.withColumn(\"has_upgraded\", df[\"has_upgraded\"].cast(\"int\"))\n",
    "df = df.withColumn(\"think_downgrade\", df[\"think_downgrade\"].cast(\"int\"))\n",
    "df = df.withColumn(\"has_downgraded\", df[\"has_downgraded\"].cast(\"int\"))\n",
    "df = df.withColumn(\"nb_404\", df[\"nb_404\"].cast(\"int\"))\n",
    "df = df.withColumn(\"Windows 8\", df[\"Windows 8\"].cast(\"int\"))\n",
    "df = df.withColumn(\"Windows 81\", df[\"Windows 81\"].cast(\"int\"))\n",
    "df = df.withColumn(\"iPad\", df[\"iPad\"].cast(\"int\"))\n",
    "df = df.withColumn(\"iPhone\", df[\"iPhone\"].cast(\"int\"))\n",
    "df = df.withColumn(\"Macintosh\", df[\"Macintosh\"].cast(\"int\"))\n",
    "df = df.withColumn(\"Linux\", df[\"Linux\"].cast(\"int\"))\n",
    "df = df.withColumn(\"Windows Vista\", df[\"Windows Vista\"].cast(\"int\"))\n",
    "df = df.withColumn(\"Windows XP\", df[\"Windows XP\"].cast(\"int\"))\n",
    "df = df.withColumn(\"Windows Seven\", df[\"Windows Seven\"].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- churn: integer (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- level: integer (nullable = true)\n",
      " |-- timedelta: integer (nullable = true)\n",
      " |-- nb_unique_songs: integer (nullable = true)\n",
      " |-- nb_total_songs: integer (nullable = true)\n",
      " |-- nb_unique_artists: integer (nullable = true)\n",
      " |-- total_length: double (nullable = true)\n",
      " |-- total_add_playlist: integer (nullable = true)\n",
      " |-- total_add_friend: integer (nullable = true)\n",
      " |-- total_thumbs_up: integer (nullable = true)\n",
      " |-- total_thumbs_down: integer (nullable = true)\n",
      " |-- total_ads: integer (nullable = true)\n",
      " |-- think_upgrade: integer (nullable = true)\n",
      " |-- has_upgraded: integer (nullable = true)\n",
      " |-- think_downgrade: integer (nullable = true)\n",
      " |-- has_downgraded: integer (nullable = true)\n",
      " |-- nb_404: integer (nullable = true)\n",
      " |-- Windows 8: integer (nullable = true)\n",
      " |-- iPad: integer (nullable = true)\n",
      " |-- iPhone: integer (nullable = true)\n",
      " |-- Macintosh: integer (nullable = true)\n",
      " |-- Linux: integer (nullable = true)\n",
      " |-- Windows Vista: integer (nullable = true)\n",
      " |-- Windows 81: integer (nullable = true)\n",
      " |-- Windows XP: integer (nullable = true)\n",
      " |-- Windows Seven: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now starts the most exciting part: **modeling**!. In the next section we will:\n",
    "* split our dataset into train and test subsets\n",
    "* build different models to compare on a specific performance metric.\n",
    "* best model will then be chosen for further tuning to see if we can improve this performance metric.  \n",
    "\n",
    "Before doing that we have two things to do:\n",
    "* decide on which metric we will measure our performance\n",
    "* choose the models that will be built\n",
    "\n",
    "## 2.1 About the performance metric and potential models\n",
    "Our data is imbalanced: we have more no churn users that churn ones so the `accuracy` cannot be taken as the performance metric. If we do not want to choose between `Precision` or `Recall` because both are kind of equally important we can choose to\n",
    "use the ***F1-Score*** metric which is the harmonic mean of both:  \n",
    "> ```F1 score = 2x ((Precision x Recall)/(Precision + Recall))```  \n",
    "\n",
    "This will be our choice!\n",
    "\n",
    "About the models, as we are in a supervised learning classification problem:\n",
    "* I will take the **Logistic Regression** as the baseline model\n",
    "* Then, more sophisticated algorithms based on trees (**Random Forest** or even **GradientBoostingTree** can be tried). Moreover, with such models we can also plot the **feature importance** so it is easier to understand (and so to explain).\n",
    "* To show that `accuracy` is not good we can also build 2 dummy classifier: an pessimistic one that will predict each user as a churn one and an optimistic one that will predict that all users will stay (so no churn at all).\n",
    "\n",
    "Transformations to do:\n",
    "* **Logistic Regression**: **features have to be scaled** (I will use MinMaxScaler so that it will be between 0 and 1). **No missing value allowed**: imputation as decided in previous notebook.\n",
    "* For **tree based solutions, there is no need to scale** but no missing value allowed neither.\n",
    "\n",
    "## 2.2. Train-test split\n",
    "Made with the help of [Spark official documentation](https://spark.apache.org/docs/latest/ml-tuning.html#train-validation-split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train, xy_test = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 rows in training dataset, 32 rows in testing dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"{} rows in training dataset, {} rows in testing dataset\".format(xy_train.count(), xy_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How \"good\" was the split? How many churn users are in both datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         avg(churn)|\n",
      "+-------------------+\n",
      "|0.22279792746113988|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xy_train.select(Fmean('churn')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(churn)|\n",
      "+----------+\n",
      "|   0.28125|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xy_test.select(Fmean('churn')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(x_cols, clf, use_scaling=False):\n",
    "    \"\"\"\n",
    "    Build the pipeline that will fit the needs\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=x_cols, outputCol=\"x_cols\")\n",
    "    minmax_scaler = MinMaxScaler(inputCol=\"x_cols\", outputCol=\"scaled_feat\")\n",
    "    stages = [assembler]\n",
    "    if use_scaling:\n",
    "        stages.append(minmax_scaler)\n",
    "    stages.append(clf)\n",
    "    return Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "evaluator_train = MulticlassClassificationEvaluator(labelCol='churn', metricName='f1')\n",
    "evaluator_test = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Dummy classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|churn|count(churn)|\n",
      "+-----+------------+\n",
      "|    1|           9|\n",
      "|    0|          23|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xy_test.groupby('churn').agg({'churn': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Pessimistic dummy classifier (everyone will churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pessimistic = xy_test.select('churn').withColumn('prediction', lit(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score for pessimistic classifier: 0.12 (accuracy: 0.28)\n"
     ]
    }
   ],
   "source": [
    "acc_score = evaluator_test.evaluate(df_pessimistic, {evaluator_test.metricName: \"accuracy\"})\n",
    "f1_score = evaluator_test.evaluate(df_pessimistic, {evaluator_test.metricName: \"f1\"})\n",
    "print(\"F1-Score for pessimistic classifier: {:.2f} (accuracy: {:.2f})\".format(f1_score, acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Optimistic dummy classifier (no one will churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimistic = xy_test.select('churn').withColumn('prediction', lit(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score for optimistic classifier: 0.60 (accuracy: 0.72)\n"
     ]
    }
   ],
   "source": [
    "acc_score = evaluator_test.evaluate(df_optimistic, {evaluator_test.metricName: \"accuracy\"})\n",
    "f1_score = evaluator_test.evaluate(df_optimistic, {evaluator_test.metricName: \"f1\"})\n",
    "print(\"F1-Score for optimistic classifier: {:.2f} (accuracy: {:.2f})\".format(f1_score, acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it is quite easy to have 72% of accuracy with a dumb classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score for Logistic Regression model: 0.78\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(featuresCol=\"scaled_feat\", labelCol=\"churn\")\n",
    "\n",
    "# No specific param for the moment, just use the default parameters\n",
    "param = ParamGridBuilder().build()\n",
    "pipeline = build_pipeline(x_cols, log_reg, True)\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator\n",
    "log_reg_cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param, evaluator=evaluator_train, numFolds=3)\n",
    "\n",
    "cv_model = log_reg_cv.fit(xy_train)\n",
    "pred = cv_model.transform(xy_test)\n",
    "\n",
    "f1_score = evaluator_test.evaluate(pred, {evaluator_test.metricName: \"f1\"})\n",
    "print(\"F1-Score for Logistic Regression model: {:.2f}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------------+\n",
      "|churn|prediction|count(prediction)|\n",
      "+-----+----------+-----------------+\n",
      "|    1|       0.0|                4|\n",
      "|    0|       0.0|               20|\n",
      "|    1|       1.0|                5|\n",
      "|    0|       1.0|                3|\n",
      "+-----+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.groupby(\"churn\", \"prediction\").agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score for Random Forest model: 0.66\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(featuresCol=\"x_cols\", labelCol=\"churn\")\n",
    "\n",
    "# No specific param for the moment, just use the default parameters\n",
    "param = ParamGridBuilder().build()\n",
    "pipeline = build_pipeline(x_cols, rf)\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator\n",
    "rf_cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param, evaluator=evaluator_train, numFolds=3)\n",
    "\n",
    "cv_model = rf_cv.fit(xy_train)\n",
    "pred = cv_model.transform(xy_test)\n",
    "\n",
    "f1_score = evaluator_test.evaluate(pred, {evaluator_test.metricName: \"f1\"})\n",
    "print(\"F1-Score for Random Forest model: {:.2f}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------------+\n",
      "|churn|prediction|count(prediction)|\n",
      "+-----+----------+-----------------+\n",
      "|    1|       0.0|                7|\n",
      "|    0|       0.0|               20|\n",
      "|    1|       1.0|                2|\n",
      "|    0|       1.0|                3|\n",
      "+-----+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.groupby(\"churn\", \"prediction\").agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. GradientBoostingTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score for GradientBoostingTree model: 0.70\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(featuresCol=\"x_cols\", labelCol=\"churn\")\n",
    "\n",
    "# No specific param for the moment, just use the default parameters\n",
    "param = ParamGridBuilder().build()\n",
    "pipeline = build_pipeline(x_cols, gbt)\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator\n",
    "gbt_cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param, evaluator=evaluator_train, numFolds=3)\n",
    "\n",
    "cv_model = gbt_cv.fit(xy_train)\n",
    "pred = cv_model.transform(xy_test)\n",
    "\n",
    "f1_score = evaluator_test.evaluate(pred, {evaluator_test.metricName: \"f1\"})\n",
    "print(\"F1-Score for GradientBoostingTree model: {:.2f}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------------+\n",
      "|churn|prediction|count(prediction)|\n",
      "+-----+----------+-----------------+\n",
      "|    1|       0.0|                3|\n",
      "|    0|       0.0|               16|\n",
      "|    1|       1.0|                6|\n",
      "|    0|       1.0|                7|\n",
      "+-----+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.groupby(\"churn\", \"prediction\").agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# CONCLUSION\n",
    "* With **this** dataset (which is small), the **best model so far is the Logistic Regression with a F1-Score of 0.78**, followed by GBT with 0.70 and then RandomForest with 0.66.  \n",
    "* It is good to note that **none of RandomForest and GBT have been tuned** (they offer more parameters to tune than Logistic Regression).\n",
    "* It is also good to note that all **those 3 models are better than our dummy classifier**. It means that in any case, no matter of the choice, any model is a better option than choosing arbitrarily.\n",
    "\n",
    "***That would be too fast and simple to make any further conclusions***. To choose a model and tune it we need more data (remember that here we have only 32 users to classify and the model has been trained with less than 200 users...we cannot really say that it is \"big data\" ;-)).  \n",
    "In this project we have the option to setup a cluster on the cloud in order to use Spark on a bigger dataset. This is what I did (follow this [notebook](APPENDIX_Sparkify_EMR.ipynb) for more details). Results have been stored and exported so that they can be loaded from here. This is what will be detailed in the next [notebook](4_Sparkify_Modeling_Full_Dataset.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
